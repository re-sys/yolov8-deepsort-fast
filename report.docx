以下是一份中期实验报告的模板，你可以根据实际情况进行调整和完善。
《特定人物视觉跟踪与手势识别中期实验报告》
一、引言
在当今科技迅速发展的时代，实现特定人物视觉跟踪和手势识别具有重要的现实意义。本实验旨在编写程序，对摄像头读取到的图片进行处理，实现特定人物的精准跟踪和特定手势的准确识别。具体目标包括：
对于特定人物，能够在图像上准确确定其大致方位，用方框进行标注，并为每个人物分配特定的 ID，即使在人物行走过程中出现遮挡情况，也能保持对同一人物的跟踪，不会跟丢。
对于手势识别，当在相机前展示特定的四种手势时，相机能够识别出手势对应的含义，并用方框框出手势出现的位置并输出其含义，而对其他手势则不作判别，视为背景处理。
二、问题描述
（一）行人检测
相机读取到 RGB 图像后，会获得一个 tensor，其中每个数据点的数值代表亮度。将这个三层 RGB 亮度矩阵数据作为输入，通过特定的检测系统，输出一个广义变长数组。数组中的每个元素是一个结构体，结构体包含边框信息（如四方框的左上点坐标和右下点坐标）以及人物 ID 信息。如果系统检测到 n 个人，那么输出的结构体就有 n 个，从而获得人物跟踪的基本信息。
（二）手势检测
同样以 RGB 图像的像素点强度信息作为输入，经过检测系统后，输出的结构体包含边框信息和识别的类型信息（例如采用编号，0 代表 palm，1 代表 fist，2 代表 left，3 代表 right）。
三、实验方法
采用 yolov8 模型
YOLOv8 算法的原理
骨干网络（Backbone）：
负责提取图像的特征，是整个网络的基础部分。
起始阶段会有一些基础的卷积层，用于初步捕捉图像中的低级特征，如边缘、纹理等。
接着是多个经过改进的 C2F（C2F = CSP + Focus）模块。C2F 模块中，会对输入的特征图进行分割、处理和融合等操作，不断提取更高级、更抽象的特征。这些特征包含了图像中不同层次的信息，既有局部的细节特征，也有全局的语义特征。通过骨干网络的处理，最终输出一系列具有丰富语义信息的特征图，为后续的检测任务提供基础。
其中CSP网络具体工作原理：
特征分流：CSP 结构把输入的特征图分成两部分。一部分经过一系列的卷积层、池化层等常规操作进行特征提取和变换。例如，这些操作可能包括 1×1 卷积用于通道数的调整，3×3 卷积用于提取局部特征等。另一部分特征图则不经过这些复杂的中间处理，直接与经过处理的那部分特征图进行拼接或融合。
特征融合：在特征提取部分完成后，将两部分特征图进行合并。这种融合方式可以是简单的拼接，即将两部分特征图在通道维度上进行拼接，使得融合后的特征图包含了来自不同处理路径的信息。也可以是更复杂的融合方式，如采用逐元素相加或其他融合策略，从而增强网络对特征的表达能力。
优势：因为没有将所有特征继续卷积，所以减少了计算量，提高计算效率。由于特征融合了输入的原始特征也包含了更抽象的进一步特征，因此有助于提供更全面的特征表示。合理的特征分流可以有效抑制梯度消失或爆炸，因为直连的一部分可以看作梯度高速公路，梯度直接通过这部分反向传播，避免经过过多卷积层和非线性激活函数。

颈部网络（Neck）：
C2F 模块的应用：YOLOv8 的颈部网络采用了 C2F（C2F = CSP + Focus）模块，取代了之前版本中的部分结构。C2F 模块结合了 CSP 结构的优点和 Focus 模块的特性。Focus 模块通过对输入图像进行切片和重新排列操作，将图像的信息集中起来，减少了信息的丢失，提高了特征的表达能力。而 C2F 模块中的跳层连接和额外的 split 操作，使得不同层次的特征能够更好地融合，增强了模型对多尺度特征的提取能力。
特征融合方式：颈部网络还采用了特征金字塔网络（FPN）和路径聚合网络（PAN）相结合的方式进行特征融合。FPN 自顶向下将高层的语义信息传递到低层，增强了低层特征的语义表达；PAN 则自底向上将低层的位置信息传递到高层，提高了高层特征的定位精度。这种双向的特征融合方式，使得 YOLOv8 能够更好地处理不同大小的物体，提高了模型的检测性能。
检测头（Head）：
解耦头结构：YOLOv8 的检测头采用了解耦头结构，将分类和回归任务分离。这种结构的好处是可以让模型更加专注于不同的任务，减少分类和回归之间的相互干扰，提高模型的准确性。在解耦头中，分类分支和回归分支分别独立地进行预测，然后将结果进行合并，得到最终的检测结果。
Anchor-free 机制：与之前的 Anchor-based 方法不同，YOLOv8 采用了 Anchor-free 的检测方式。Anchor-based 方法需要预先定义一系列的锚框，然后根据锚框与目标的匹配程度来进行预测。而 Anchor-free 方法直接预测目标的中心点坐标和边界框的尺寸，减少了对锚框的依赖，降低了计算复杂度，同时也提高了模型对不同形状和大小物体的检测能力。
损失函数（Loss Function）：
正负样本分配策略：YOLOv8 采用了 TaskAlignedAssigner 正样本分配策略。该策略根据分类与回归的分数加权得到一个对齐分数，然后根据对齐分数选择正样本。这种动态的正负样本分配策略，能够更好地适应不同的目标场景，提高模型的训练效果和检测性能。
损失计算方式：在损失计算方面，YOLOv8 的分类分支采用 BCE Loss（Binary Cross Entropy Loss，二元交叉熵损失），用于计算分类的误差；回归分支则使用 DistributionFocal Loss 和 CIoU Loss。DistributionFocal Loss 是一种针对目标检测中边界框回归问题的损失函数，它考虑了边界框的分布情况，能够更好地处理边界框的不确定性；CIoU Loss 则考虑了边界框的重叠面积、中心点距离和长宽比等因素，使得回归的结果更加准确。进行预测推理图片的内容。该模型基于卷积神经网络、残差网络等神经网络，具有强大的图像识别能力。
使用自己制作的数据集进行模型训练，以提高模型对特定人物和手势的识别准确率。
在人物检测中，对人物方位进行推理识别后，采用 deepsort 方法对框选出来的人物进行再次特征识别、IOU 匹配和 Kalman 滤波预测，从而为每一个框选出来的人物分配特定 ID，实现人物跟踪。
四、实验过程与结果
（一）实验准备
在宿舍附近拍摄六段视频作为训练的数据集，并从测试集中每 10 帧提取一张图片。手动标注图片超过 1000 张，为模型训练提供了丰富的标注数据。
（二）模型训练
参考 roboflow 的教程进行模型训练，首先将数据大小统一，然后进行数据增强，如 clip、flip、saturation 变换等，以增加数据的多样性和模型的泛化能力。
（三）实验结果
训练后的模型得到了 confusion matrix 等图片。对原始的运行代码进行了 openvino 的模型导出和使用，充分利用显卡加速推理，取得了较好的效果。同时，也尝试了用 C++ 运行相同模型，发现使用 onnx 模型时，对同样的视频推理效果会变差。在其他场景也拍摄了类似视频进行测试，在特定场景下，手部识别和人物追踪都有较好的效果。
